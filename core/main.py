import os
import re
import time
import threading
from collections import defaultdict
from typing import List
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationalRetrievalChain
from google.api_core.exceptions import ResourceExhausted

# load env & HF caches
load_dotenv()

# --- HF cache & model setup 
HF_CACHE_DIR = os.getenv("HF_CACHE_DIR", "./hf_cache")
HF_LOCAL_MODEL_DIR = os.getenv("HF_LOCAL_MODEL_DIR", "")  # optional: "./models/all-MiniLM-L6-v2"
HF_MODEL_NAME = os.getenv("HF_MODEL_NAME", "sentence-transformers/paraphrase-MiniLM-L3-v2")
CHROMA_PERSIST_DIR = os.getenv("CHROMA_PERSIST_DIR", "./chromadb_wbc_usa")

os.makedirs(HF_CACHE_DIR, exist_ok=True)
# set env vars so HF/transformers use persistent cache (important on Render)
os.environ["HF_HOME"] = HF_CACHE_DIR
os.environ["TRANSFORMERS_CACHE"] = HF_CACHE_DIR
os.environ["HF_DATASETS_CACHE"] = HF_CACHE_DIR

# GOOGLE API key for Gemini
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY", "")

# user memory
user_memory_store = {}
user_last_player = {}

# players list
all_players = [
    "Brady Singer", "Lance Lynn", "Devin Williams", "Adam Wainwright",
    "Daniel Bard", "Jason Adam", "David Bednar", "Nick Martinez",
    "Miles Mikolas", "Kendall Graveman", "Ryan Pressly", "Aaron Loup",
    "Kyle Freeland", "Adam Ottavino", "Merrill Kelly"
]

# lazy globals
embedding = None
vectordb = None
_vectordb_lock = None

# prompt
template = """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ£’çƒæƒ…è’åˆ†æå¸«ï¼Œè«‹æ ¹æ“šç¾åœ‹éšŠæŠ•æ‰‹çš„ 2022 å¹´è³‡æ–™ï¼Œå°ä½¿ç”¨è€…çš„å•é¡Œå…¨é¢åˆ†æèˆ‡èªªæ˜ã€‚

ã€å…§å®¹åŸå‰‡ã€‘
- çµè«–å„ªå…ˆï¼Œè«‹å…ˆæ‘˜è¦å‡ºé‡é»ç¸½çµæˆ–å»ºè­°ï¼ˆå¯æ¢åˆ—ï¼‰ï¼Œè®“è®€è€…èƒ½å¿«é€ŸæŒæ¡æ ¸å¿ƒè³‡è¨Š
- åƒ…ä¾æ“šæä¾›çš„å…§å®¹å›ç­”ï¼Œ**ä¸å¾—æé€ ä»»ä½•æœªå­˜åœ¨çš„è³‡è¨Š**
- å¦‚æœ‰éœ€é€²è¡Œæ¨è«–çš„å¿…è¦æ€§ï¼Œ**è«‹æ˜ç¢ºæŒ‡å‡ºå±¬æ–¼æ¨è«–çš„éƒ¨åˆ†**
- å›ç­”æ¸…æ¥šã€å°ˆæ¥­ã€æ˜“æ‡‚
- å›ç­”å¾Œçš„å…§å®¹ä¾æ“šå¾ç°¡é™„è¨»

ã€è³‡æ–™ç´€éŒ„ã€‘
{context}

ã€å•é¡Œã€‘
{question}

ã€è«‹è¼¸å‡ºä½ çš„å›ç­”ã€‘
"""
prompt = PromptTemplate(template=template, input_variables=["context", "question"])

def init_vectordb_if_needed():
    global embedding, vectordb, _vectordb_lock
    if _vectordb_lock is None:
        _vectordb_lock = threading.Lock()
    with _vectordb_lock:
        if vectordb is not None:
            return

        print("ğŸ”„ åˆæ¬¡è¼‰å…¥å‘é‡åº«ä¸­ï¼ˆlazy initï¼‰...")

        try:
            embedding = HuggingFaceEndpointEmbeddings(
                model=os.getenv("HF_MODEL_NAME", "sentence-transformers/all-MiniLM-L6-v2"),
                huggingfacehub_api_token=os.getenv("HF_API_TOKEN")
            )
        except Exception as e:
            # make error explicit and re-raise for caller to catch and push to LINE
            print("âŒ HuggingFaceEmbeddings åˆå§‹åŒ–å¤±æ•—:", e)
            raise RuntimeError(f"HuggingFaceEmbeddings init failed: {e}")

        try:
            # If Chroma DB folder exists -> load; otherwise try to load but warn (prefill recommended)
            if os.path.exists(CHROMA_PERSIST_DIR) and os.listdir(CHROMA_PERSIST_DIR):
                vectordb = Chroma(persist_directory=CHROMA_PERSIST_DIR, embedding_function=embedding)
                print("âœ… å·²è¼‰å…¥ç¾æœ‰å‘é‡åº«")
            else:
                # å¦‚æœæ²’æœ‰é å…ˆå»ºå¥½çš„ DBï¼Œå…ˆå˜—è©¦è¼‰å…¥ï¼‰
                print("âš ï¸ chroma persist dir ç©ºæˆ–ä¸å­˜åœ¨ï¼Œæœƒåœ¨ç¬¬ä¸€æ¬¡ run æ™‚å»ºç«‹ã€‚å»ºè­°é å…ˆå»ºç«‹ä»¥é¿å… cold-start å»ºåº«å»¶é²ã€‚")
                vectordb = Chroma(embedding_function=embedding, persist_directory=CHROMA_PERSIST_DIR)
                print("â„¹ï¸ å·²å»ºç«‹ Chroma handleï¼ˆä½†æœªæ–°å¢ documentsï¼‰ã€‚è‹¥å‘é‡åº«ç‚ºç©ºï¼Œæª¢ç´¢å°‡æ‰¾ä¸åˆ°æ–‡ä»¶ã€‚")
        except Exception as e:
            print("âŒ Chroma è¼‰å…¥/å»ºç«‹å¤±æ•—:", e)
            raise RuntimeError(f"Chroma init failed: {e}")

        print("âœ… å‘é‡åº«è¼‰å…¥å®Œæˆ")

def extract_player_name(question: str, all_players: List[str]) -> List[str]:
    matched = []
    q_lower = question.lower()
    for full_name in all_players:
        if full_name.lower() in q_lower:
            matched.append(full_name)
    if matched:
        print(f"ğŸ¯ å•é¡Œä¸­æ˜ç¢ºæŒ‡å®šçƒå“¡ï¼š{matched}")
        return matched

    words = re.findall(r"[a-zA-Z]+", question)
    if len(words) == 1:
        last_candidate = words[0].lower()
        for full_name in all_players:
            _, last_name = map(str.lower, full_name.split())
            if last_candidate == last_name:
                matched.append(full_name)
        if matched:
            print(f"ğŸ¯ å•é¡Œä¸­ä»¥å§“æ°åˆ¤æ–·çƒå“¡ï¼š{matched}")
    return matched

def estimate_token_count(text: str) -> int:
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    non_chinese = len(text) - chinese_chars
    return int(chinese_chars * 1.2 + non_chinese * 0.75)

def get_answer(question: str, player_name: list = None, user_id: str = "default") -> str:
    # lazy init vectordb
    try:
        init_vectordb_if_needed()
    except Exception as e:
        err = f"âŒ åˆå§‹åŒ–å‘é‡åº«å¤±æ•—: {e}"
        print(err)
        return err

    extracted_players = extract_player_name(question, all_players)
    if extracted_players:
        player_name = extracted_players
    else:
        if user_id in user_memory_store:
            memory = user_memory_store[user_id]
            history = memory.load_memory_variables({})["chat_history"]
            if history:
                for msg in reversed(history[-4:]):
                    names = extract_player_name(msg.content, all_players)
                    if names:
                        player_name = names
                        print(f"ğŸ§  å¾è¨˜æ†¶è£œè¶³ player_name: {player_name}")
                        break

    if user_id in user_last_player:
        if sorted(player_name or []) != sorted(user_last_player[user_id]):
            print(f"ğŸ”„ åµæ¸¬åˆ°çƒå“¡åˆ‡æ›ï¼Œé‡ç½® {user_id} è¨˜æ†¶ã€‚å¾ {user_last_player[user_id]} -> {player_name}")
            try:
                del user_memory_store[user_id]
            except KeyError:
                pass
            user_last_player[user_id] = player_name or []
    else:
        user_last_player[user_id] = player_name or []

    MAX_TOKENS = 125000
    MAX_K = 20
    MIN_K = 1
    if player_name:
        num_players = len(player_name)
        k_per_player = max(1, MAX_K // num_players)
    else:
        k_per_player = MAX_K

    print(f"âš¾ï¸ æŠ½å–çƒå“¡ï¼š{player_name if player_name else 'æœªæŒ‡å®š'}ï¼Œæ¯äººæœ€å¤šå– {k_per_player} ç­†")

    low = MIN_K
    high = k_per_player
    best_k = None
    best_context_text = ""

    while low <= high:
        mid = (low + high) // 2
        search_kwargs = {"k": mid}
        if player_name:
            search_kwargs["filter"] = {"player_name": {"$in": player_name}}

        temp_retriever = vectordb.as_retriever(search_kwargs=search_kwargs)
        try:
            docs = temp_retriever.invoke(question)
        except Exception as e:
            print(f"æª¢ç´¢æ™‚ç™¼ç”Ÿä¾‹å¤–: {e}")
            docs = []

        if not docs:
            print(f"â—ï¸ k={mid} ç„¡æª¢ç´¢åˆ°æ–‡ä»¶ï¼Œå¾€æ›´å¤§ k å˜—è©¦")
            low = mid + 1
            continue

        context_text = "\n\n".join(doc.page_content for doc in docs)
        estimated_tokens = estimate_token_count(context_text) + estimate_token_count(question)
        print(f"ğŸ§® é ä¼° tokens: {estimated_tokens} (k={mid})")

        if estimated_tokens <= MAX_TOKENS:
            best_k = mid
            best_context_text = context_text
            print(f"âœ… k={mid} ç¬¦åˆé™åˆ¶ï¼Œå˜—è©¦æ›´å¤§ k")
            low = mid + 1
        else:
            print(f"âŒ k={mid} è¶…é token é™åˆ¶ï¼Œå˜—è©¦æ›´å° k")
            high = mid - 1

    if best_k is None:
        return "âš ï¸ æ‰¾ä¸åˆ°ç¬¦åˆ token é™åˆ¶æˆ–å‘é‡åº«æ²’æœ‰ç›¸é—œæ–‡ä»¶ã€‚"

    print(f"ğŸ” æœ€çµ‚é¸æ“‡ k={best_k} é€²è¡Œå›ç­”ç”Ÿæˆ")

    search_kwargs_final = {"k": best_k}
    if player_name:
        search_kwargs_final["filter"] = {"player_name": {"$in": player_name}}

    retriever = vectordb.as_retriever(search_kwargs=search_kwargs_final)

    # memory init
    if user_id not in user_memory_store:
        print(f"ğŸ”° ç‚ºä½¿ç”¨è€… {user_id} å»ºç«‹æ–°çš„è¨˜æ†¶æ± ")
        memory = ConversationSummaryBufferMemory(
            llm=ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0),
            memory_key="chat_history",
            return_messages=True
        )
        user_memory_store[user_id] = memory
    else:
        print(f"ğŸ—„ï¸ ä½¿ç”¨è€… {user_id} ä½¿ç”¨èˆŠæœ‰è¨˜æ†¶æ± ")
        memory = user_memory_store[user_id]

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
    )

    for attempt in range(9):
        try:
            print(f"ğŸš€ å•é¡Œï¼š{question}ï¼ˆPlayer: {player_name}ï¼‰ ç¬¬ {attempt+1} æ¬¡å˜—è©¦")
            result = qa_chain.invoke({"question": question})
            answer = result.get("answer", "") if isinstance(result, dict) else ""
            if not answer or not answer.strip():
                print("âš ï¸ å›ç­”ç‚ºç©ºï¼Œç¨ç­‰ 3 ç§’å†è©¦")
                time.sleep(3)
                continue
            print("âœ… æˆåŠŸå–å¾—å›ç­”")
            return answer
        except ResourceExhausted:
            print(f"âš ï¸ API é…é¡é™åˆ¶ï¼Œç­‰å¾… 61 ç§’å¾Œé‡è©¦...ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰")
            time.sleep(61)
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
    return "âŒ å¤šæ¬¡å˜—è©¦ä»å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–æª¢æŸ¥é…é¡ã€‚"
'''
# test
if __name__ == "__main__":
    test_questions = [
        ("Singerçš„æ§çƒå¦‚ä½•ï¼Ÿ", "user_test1"),
        ("lynnçš„çƒè·¯å“è³ªå¦‚ä½•ï¼Ÿ", "user_test2"),
        ("Devin Williams 2022 æœ€å¸¸ç”¨çƒç¨®ï¼Ÿ", "user_test3"),
        ("Pressly çš„æ•‘æ´æˆåŠŸç‡ï¼Ÿ", "user_test4"),
        ("Mikolas è¡¨ç¾åˆ†æ", "user_test5"),
    ]

    for q, uid in test_questions:
        print("="*50)
        print(f"ğŸ”¹ æ¸¬è©¦å•é¡Œï¼š{q} (ä½¿ç”¨è€…: {uid})")
        ans = get_answer(q, user_id=uid)
        print(f"ğŸ’¡ å›ç­”ï¼š{ans}\n")
'''