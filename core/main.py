import os
import re
import time
import uvicorn

from collections import defaultdict
from typing import List
from dotenv import load_dotenv
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationalRetrievalChain
from google.api_core.exceptions import ResourceExhausted

# ç’°å¢ƒè®Šæ•¸
load_dotenv()
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")

# FastAPI app
app = FastAPI()

# ä½¿ç”¨è€…è¨˜æ†¶æ± 
user_memory_store = {}
user_last_player = {}

# çƒå“¡åå–®
all_players = [
    "Brady Singer", "Lance Lynn", "Devin Williams", "Adam Wainwright",
    "Daniel Bard", "Jason Adam", "David Bednar", "Nick Martinez",
    "Miles Mikolas", "Kendall Graveman", "Ryan Pressly", "Aaron Loup",
    "Kyle Freeland", "Adam Ottavino", "Merrill Kelly"
]

# lazy vector DB handle
embedding = None
vectordb = None
_vectordb_lock = None

# Prompt template
template = """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ£’çƒæƒ…è’åˆ†æå¸«ï¼Œè«‹æ ¹æ“šç¾åœ‹éšŠæŠ•æ‰‹çš„ 2022 å¹´è³‡æ–™ï¼Œå°ä½¿ç”¨è€…çš„å•é¡Œå…¨é¢åˆ†æèˆ‡èªªæ˜ã€‚

ã€å…§å®¹åŸå‰‡ã€‘
- çµè«–å„ªå…ˆï¼Œè«‹å…ˆæ‘˜è¦å‡ºé‡é»ç¸½çµæˆ–å»ºè­°ï¼ˆå¯æ¢åˆ—ï¼‰ï¼Œè®“è®€è€…èƒ½å¿«é€ŸæŒæ¡æ ¸å¿ƒè³‡è¨Š
- åƒ…ä¾æ“šæä¾›çš„å…§å®¹å›ç­”ï¼Œ**ä¸å¾—æé€ ä»»ä½•æœªå­˜åœ¨çš„è³‡è¨Š**
- å¦‚æœ‰éœ€é€²è¡Œæ¨è«–çš„å¿…è¦æ€§ï¼Œ**è«‹æ˜ç¢ºæŒ‡å‡ºå±¬æ–¼æ¨è«–çš„éƒ¨åˆ†**
- å›ç­”æ¸…æ¥šã€å°ˆæ¥­ã€æ˜“æ‡‚ï¼Œé©åˆçƒå“¡ã€æ•™ç·´èˆ‡ç®¡ç†å±¤é–±è®€
- å›ç­”å¾Œçš„å…§å®¹ä¾æ“šå¾ç°¡é™„è¨»

ã€è³‡æ–™ç´€éŒ„ã€‘
{context}

ã€å•é¡Œã€‘
{question}

ã€è«‹è¼¸å‡ºä½ çš„å›ç­”ã€‘
"""
prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)
def init_vectordb_if_needed():
    """Lazy init embedding and vectordb (first call only)."""
    global embedding, vectordb, _vectordb_lock
    if _vectordb_lock is None:
        import threading
        _vectordb_lock = threading.Lock()
    with _vectordb_lock:
        if vectordb is None:
            print("ğŸ”„ åˆæ¬¡è¼‰å…¥å‘é‡åº«ä¸­ï¼ˆlazy initï¼‰...")
            embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
            # persist_directory èˆ‡ä½ çš„åŸå§‹è¨­å®šä¸€è‡´
            vectordb = Chroma(persist_directory="./chromadb_wbc_usa", embedding_function=embedding)
            print("âœ… å‘é‡åº«è¼‰å…¥å®Œæˆ")

# åŠŸèƒ½å‡½æ•¸ï¼ˆä¿æŒåŸæœ‰é‚è¼¯ï¼Œåƒ…åœ¨éœ€è¦æ™‚æ‰ init vectordbï¼‰
def extract_player_name(question: str, all_players: List[str]) -> List[str]:
    matched = []
    q_lower = question.lower()
    for full_name in all_players:
        if full_name.lower() in q_lower:
            matched.append(full_name)
    if matched:
        print(f"ğŸ¯ å•é¡Œä¸­æ˜ç¢ºæŒ‡å®šçƒå“¡ï¼š{matched}")
        return matched

    words = re.findall(r"[a-zA-Z]+", question)
    if len(words) == 1:
        last_candidate = words[0].lower()
        for full_name in all_players:
            _, last_name = map(str.lower, full_name.split())
            if last_candidate == last_name:
                matched.append(full_name)
        if matched:
            print(f"ğŸ¯ å•é¡Œä¸­ä»¥å§“æ°åˆ¤æ–·çƒå“¡ï¼š{matched}")
    return matched

def estimate_token_count(text: str) -> int:
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    non_chinese = len(text) - chinese_chars
    return int(chinese_chars * 1.2 + non_chinese * 0.75)

def get_answer(question: str, player_name: list = None, user_id: str = "default") -> str:
    # lazy init vectordb on first real request
    init_vectordb_if_needed()

    extracted_players = extract_player_name(question, all_players)
    if extracted_players:
        player_name = extracted_players
    else:
        if user_id in user_memory_store:
            memory = user_memory_store[user_id]
            history = memory.load_memory_variables({})["chat_history"]
            if history:
                for msg in reversed(history[-4:]):
                    names = extract_player_name(msg.content, all_players)
                    if names:
                        player_name = names
                        print(f"ğŸ§  å¾è¨˜æ†¶è£œè¶³ player_name: {player_name}")
                        break

    if user_id in user_last_player:
        if sorted(player_name or []) != sorted(user_last_player[user_id]):
            print(f"ğŸ”„ åµæ¸¬åˆ°çƒå“¡åˆ‡æ›ï¼Œé‡ç½® {user_id} è¨˜æ†¶ã€‚å¾ {user_last_player[user_id]} -> {player_name}")
            try:
                del user_memory_store[user_id]
            except KeyError:
                pass
            user_last_player[user_id] = player_name or []
    else:
        user_last_player[user_id] = player_name or []

    MAX_TOKENS = 225000
    MAX_K = 20
    MIN_K = 1

    if player_name:
        num_players = len(player_name)
        k_per_player = max(1, MAX_K // num_players)
    else:
        k_per_player = MAX_K

    print(f"âš¾ï¸ æŠ½å–çƒå“¡ï¼š{player_name if player_name else 'æœªæŒ‡å®š'}ï¼Œæ¯äººæœ€å¤šå– {k_per_player} ç­†")

    low = MIN_K
    high = k_per_player
    best_k = None
    best_context_text = ""

    while low <= high:
        mid = (low + high) // 2
        search_kwargs = {"k": mid}
        if player_name:
            search_kwargs["filter"] = {"player_name": {"$in": player_name}}

        temp_retriever = vectordb.as_retriever(search_kwargs=search_kwargs)
        docs = temp_retriever.invoke(question)
        if not docs:
            print(f"â—ï¸ k={mid} ç„¡æª¢ç´¢åˆ°æ–‡ä»¶ï¼Œå¾€æ›´å¤§ k å˜—è©¦")
            low = mid + 1
            continue

        context_text = "\n\n".join(doc.page_content for doc in docs)
        estimated_tokens = estimate_token_count(context_text) + estimate_token_count(question)
        print(f"ğŸ§® é ä¼° tokens: {estimated_tokens} (k={mid})")

        if estimated_tokens <= MAX_TOKENS:
            best_k = mid
            best_context_text = context_text
            print(f"âœ… k={mid} ç¬¦åˆé™åˆ¶ï¼Œå˜—è©¦æ›´å¤§ k")
            low = mid + 1
        else:
            print(f"âŒ k={mid} è¶…é token é™åˆ¶ï¼Œå˜—è©¦æ›´å° k")
            high = mid - 1

    if best_k is None:
        return "âš ï¸ æ‰¾ä¸åˆ°ç¬¦åˆ token é™åˆ¶çš„è³‡æ–™ã€‚"

    print(f"ğŸ” æœ€çµ‚é¸æ“‡ k={best_k} é€²è¡Œå›ç­”ç”Ÿæˆ")

    search_kwargs_final = {"k": best_k}
    if player_name:
        search_kwargs_final["filter"] = {"player_name": {"$in": player_name}}

    retriever = vectordb.as_retriever(search_kwargs=search_kwargs_final)

    if user_id not in user_memory_store:
        print(f"ğŸ”° ç‚ºä½¿ç”¨è€… {user_id} å»ºç«‹æ–°çš„è¨˜æ†¶æ± ")
        memory = ConversationSummaryBufferMemory(
            llm=ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0),
            memory_key="chat_history",
            return_messages=True
        )
        user_memory_store[user_id] = memory
    else:
        print(f"ğŸ—„ï¸ ä½¿ç”¨è€… {user_id} ä½¿ç”¨èˆŠæœ‰è¨˜æ†¶æ± ")
        memory = user_memory_store[user_id]

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
    )
    for attempt in range(9):
        try:
            print(f"ğŸš€ å•é¡Œï¼š{question}ï¼ˆPlayer: {player_name}ï¼‰ ç¬¬ {attempt+1} æ¬¡å˜—è©¦")
            result = qa_chain.invoke({"question": question})
            answer = result["answer"]
            if not answer.strip():
                print("âš ï¸ å›ç­”ç‚ºç©ºï¼Œç¨ç­‰ 3 ç§’å†è©¦")
                time.sleep(3)
                continue
            print("âœ… æˆåŠŸå–å¾—å›ç­”")
            return answer
        except ResourceExhausted:
            print(f"âš ï¸ API é…é¡é™åˆ¶ï¼Œç­‰å¾… 61 ç§’å¾Œé‡è©¦...ï¼ˆç¬¬ {attempt+1} æ¬¡ï¼‰")
            time.sleep(61)
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
    return "âŒ å¤šæ¬¡å˜—è©¦ä»å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–æª¢æŸ¥é…é¡ã€‚"

# FastAPI API
class QuestionRequest(BaseModel):
    question: str
    user_id: str = "default"

@app.post("/ask")
def ask_question(req: QuestionRequest):
    print(f"ğŸ“¥ æ”¶åˆ°å•é¡Œï¼š{req.question} ä¾†è‡ªä½¿ç”¨è€…ï¼š{req.user_id}")
    answer = get_answer(req.question, user_id=req.user_id)
    print(f"ğŸ“¤ å›ç­”å®Œæˆï¼Œå›å‚³ä½¿ç”¨è€…ï¼š{req.user_id}")
    return {"answer": answer}

@app.get("/")
def home():
    return {"status": "ok", "message": "Baseball RAG API is running."}

# å•Ÿå‹•æœå‹™ï¼ˆè‹¥ç›´æ¥è·‘ main.pyï¼‰
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    print(f"ğŸš€ å•Ÿå‹• API æœå‹™ï¼ŒåŸ è™Ÿï¼š{port}")
    uvicorn.run(app, host="0.0.0.0", port=port)
