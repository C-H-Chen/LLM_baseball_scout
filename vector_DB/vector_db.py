import mysql.connector
import pandas as pd
import os
from tqdm import tqdm
from dotenv import load_dotenv

from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document

# SQLè¨­å®š
load_dotenv()
DB_CONFIG = {
    'host': os.getenv("DB_HOST"),
    'port': int(os.getenv("DB_PORT", 4000)),
    'user': os.getenv("DB_USER"),
    'password': os.getenv("DB_PASSWORD"),
    'database': os.getenv("DB_DATABASE"),
    'charset': 'utf8mb4',
    'ssl_ca': os.getenv("DB_SSL_CA"),
    'ssl_verify_cert': os.getenv("DB_SSL_VERIFY_CERT", "True") == "True",
    'ssl_verify_identity': os.getenv("DB_SSL_VERIFY_IDENTITY", "True") == "True",
    'connection_timeout': 10
}
TABLE_NAME = "pitching_data"
TEXT_FILE = "wbc_usa_pitchers_2022.txt"
PERSIST_DIR = "./chromadb_wbc_usa"

# è®€å–è³‡æ–™
conn = mysql.connector.connect(**DB_CONFIG)
query = f"SELECT * FROM {TABLE_NAME} ORDER BY player_name, game_date ASC"
df = pd.read_sql(query, conn)
conn.close()
print(f"âœ… å·²è®€å–è³‡æ–™ï¼Œå…± {len(df)} ç­†ç´€éŒ„ï¼Œ{df['player_name'].nunique()} ä½çƒå“¡ã€‚")

# åˆ†å‰²chunkä¸¦ç”Ÿæˆè‡ªç„¶èªè¨€æè¿°
def split_by_player_and_game_with_metadata(df, text_file_path=None):
    chunks = []
    full_text_output = []

    grouped = df.groupby(['player_name', 'game_date'])
    for (player_name, game_date), group in grouped:
        content_lines = []
        for _, row in group.iterrows():
            line = []
            for col in df.columns:
                if pd.notnull(row[col]):
                    val = round(row[col], 2) if isinstance(row[col], float) else row[col]
                    line.append(f"{col.replace('_', ' ')}: {val}")
            content_lines.append(" | ".join(line))

            # ç”¢ç”Ÿæ–‡å­—æª”æ ¼å¼
            text_items = []
            for col in df.columns:
                if pd.notnull(row[col]):
                    value = round(row[col], 2) if isinstance(row[col], float) else row[col]
                    text_items.append(f"{col.replace('_', ' ')} was {value}")
            line_text = f"Pitch event for {row['player_name']}: " + "; ".join(text_items) + "."
            full_text_output.append(line_text)

        full_content = "\n".join(content_lines)
        chunk_text = f"ã€çƒå“¡ï¼š{player_name}ã€‘ã€æ¯”è³½æ—¥æœŸï¼š{game_date}ã€‘\n{full_content}"
        metadata = {
            "player_name": player_name,
            "game_date": str(game_date)
        }
        chunks.append(Document(page_content=chunk_text, metadata=metadata))

    # å¯«å‡ºæ–‡å­—æª”
    if text_file_path:
        with open(text_file_path, "w", encoding="utf-8") as f:
            f.write("\n".join(full_text_output))
        print(f"âœ… å·²è¼¸å‡ºç‚º {text_file_path}")

    return chunks

# å‘¼å«ä¸Šé¢å‡½å¼ï¼ŒåŒæ™‚åˆ‡ chunk ä¸¦ç”¢å‡ºè‡ªç„¶èªè¨€æ–‡å­—æª”
documents = split_by_player_and_game_with_metadata(df, text_file_path=TEXT_FILE)

# è¨­å®šEmbedding
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")
embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

# å»ºç«‹æˆ–è¼‰å…¥å‘é‡åº«
if os.path.exists(PERSIST_DIR) and os.listdir(PERSIST_DIR):
    vectordb = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding)
    print("ğŸ“ å·²è¼‰å…¥ç¾æœ‰å‘é‡åº«ï¼Œä¸é‡è¤‡æ–°å¢æ–‡ä»¶")
else:
    vectordb = Chroma(embedding_function=embedding, persist_directory=PERSIST_DIR)
    print("ğŸ†• å°šç„¡å‘é‡åº«ï¼Œé–‹å§‹åˆ†æ‰¹æ–°å¢ documents...")

    BATCH_SIZE = 500
    for i in tqdm(range(0, len(documents), BATCH_SIZE), desc="ğŸ”„ åŠ å…¥ä¸­"):
        batch = documents[i:i + BATCH_SIZE]
        vectordb.add_documents(batch)

    print("âœ… å·²å®Œæˆå‘é‡åº«å»ºç«‹ä¸¦å„²å­˜ã€‚")

# æª¢æŸ¥æ–‡ä»¶æ•¸é‡
docs = vectordb.get()
print(f"âœ… å‘é‡åº«æ–‡ä»¶ç¸½æ•¸ï¼š{len(docs['ids'])}")
